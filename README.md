# Exact Diagonalization C++ Toolkit

A high-performance toolkit for solving quantum lattice models through exact
diagonalization of spin Hamiltonians. Computes ground states, finite-temperature
thermodynamics, and dynamical/static response functions with support for GPU
acceleration and Numerical Linked Cluster Expansion (NLCE) workflows.

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Project Structure](#project-structure)
3. [Installation](#installation)
4. [Exact Diagonalization Pipeline](#exact-diagonalization-pipeline)
   - [Solver Methods](#solver-methods)
   - [Command-Line Interface](#command-line-interface)
   - [Configuration Files](#configuration-files)
   - [Input File Formats](#input-file-formats)
   - [Output Files](#output-files)
5. [NLCE Workflow](#nlce-workflow)
   - [Overview](#nlce-overview)
   - [Running NLCE Calculations](#running-nlce-calculations)
   - [NLCE with FTLM](#nlce-with-ftlm)
   - [Analysis and Fitting](#analysis-and-fitting)
6. [Advanced Topics](#advanced-topics)
7. [Python Utilities](#python-utilities)
8. [License](#license)

---

## Quick Start

```bash
# 1. Build the toolkit
mkdir build && cd build
cmake -DWITH_CUDA=OFF -DWITH_MPI=ON ..
make -j8

# 2. Run a basic ED calculation
./ED /path/to/hamiltonian --method=LANCZOS --eigenvalues=6 --thermo

# 3. Run a complete NLCE workflow (from workflows/nlce/run/)
python3 nlce.py --max_order=4 --Jxx=1.0 --Jyy=1.0 --Jzz=1.0 --thermo
```

---

## Project Structure

```
exact_diagonalization_cpp/
‚îú‚îÄ‚îÄ include/ed/               # Public C++ headers
‚îÇ   ‚îú‚îÄ‚îÄ core/                 # Hamiltonian, configuration, types
‚îÇ   ‚îú‚îÄ‚îÄ solvers/              # Lanczos, FTLM, TPQ, ARPACK interfaces
‚îÇ   ‚îú‚îÄ‚îÄ io/                   # HDF5, basis storage
‚îÇ   ‚îî‚îÄ‚îÄ gpu/                  # CUDA wrappers and GPU kernels
‚îú‚îÄ‚îÄ src/                      # Implementation sources
‚îÇ   ‚îú‚îÄ‚îÄ apps/                 # Entry points: ed_main.cpp, TPQ_DSSF.cpp
‚îÇ   ‚îú‚îÄ‚îÄ core/                 # Core implementations
‚îÇ   ‚îú‚îÄ‚îÄ solvers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cpu/              # CPU solver implementations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gpu/              # CUDA implementations
‚îÇ   ‚îî‚îÄ‚îÄ io/                   # I/O implementations
‚îú‚îÄ‚îÄ python/edlib/             # Python utilities package
‚îÇ   ‚îú‚îÄ‚îÄ helper_cluster.py     # Hamiltonian preparation for clusters
‚îÇ   ‚îú‚îÄ‚îÄ helper_pyrochlore.py  # Pyrochlore lattice utilities
‚îÇ   ‚îú‚îÄ‚îÄ hdf5_io.py            # HDF5 I/O utilities
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ workflows/nlce/           # NLCE workflow scripts
‚îÇ   ‚îú‚îÄ‚îÄ prep/                 # Cluster generation
‚îÇ   ‚îú‚îÄ‚îÄ run/                  # ED execution and NLCE summation
‚îÇ   ‚îî‚îÄ‚îÄ analysis/             # Fitting and convergence analysis
‚îú‚îÄ‚îÄ scripts/                  # Plotting and utility scripts
‚îú‚îÄ‚îÄ docs/                     # Extended documentation
‚îú‚îÄ‚îÄ examples/                 # Sample configuration files
‚îú‚îÄ‚îÄ data/                     # Input data files
‚îú‚îÄ‚îÄ results/                  # Output directory (gitignored)
‚îî‚îÄ‚îÄ CMakeLists.txt            # Build configuration
```

---

## Installation

### Prerequisites

| Component | Required | Notes |
|-----------|----------|-------|
| C++17 compiler | ‚úÖ | GCC ‚â•9, Clang ‚â•10, or MSVC ‚â•2019 |
| CMake | ‚úÖ | Version 3.18+ |
| BLAS/LAPACK | ‚úÖ | OpenBLAS, MKL, AOCL BLIS, or system |
| Eigen3 | ‚úÖ | Header-only linear algebra |
| HDF5 | ‚úÖ | For data I/O |
| ARPACK | ‚úÖ | Sparse eigenvalue solver |
| CUDA | ‚ùå | Optional GPU acceleration |
| MPI | ‚ùå | Optional distributed computing |
| Python 3.8+ | ‚ùå | For NLCE workflows and plotting |

### Build Options

```bash
mkdir build && cd build

# CPU-only build (default)
cmake -DWITH_CUDA=OFF -DWITH_MPI=OFF ..

# With GPU support
cmake -DWITH_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=80 ..

# With MPI for distributed TPQ
cmake -DWITH_MPI=ON ..

# With Intel MKL (auto-detected on Intel CPUs)
cmake -DWITH_MKL=ON ..

# With AMD AOCL BLIS
cmake -DUSE_AOCL_BLIS=ON ..

# Build
cmake --build . --target ED TPQ_DSSF -j$(nproc)
```

### Python Dependencies

```bash
pip install numpy scipy matplotlib h5py networkx tqdm
# Optional for fitting:
pip install scikit-optimize
```

---

## Exact Diagonalization Pipeline

The ED pipeline computes eigenvalues and eigenvectors of quantum spin
Hamiltonians, then derives thermodynamic properties and response functions.

### Solver Methods

| Method | Description | Best For |
|--------|-------------|----------|
| `FULL` | Full diagonalization (LAPACK) | Small systems (‚â§16 sites) |
| `LANCZOS` | Iterative ground state | Ground state + few excited |
| `ARPACK` | Sparse eigenvalue solver | Multiple eigenvalues |
| `ARPACK_ADVANCED` | ARPACK with auto-tuning | Difficult convergence |
| `FTLM` | Finite-Temperature Lanczos | Thermodynamics (moderate T) |
| `LTLM` | Low-Temperature Lanczos | Thermodynamics (low T) |
| `HYBRID` | LTLM (low T) + FTLM (high T) | Full temperature range |
| `mTPQ` | Microcanonical TPQ | Large systems, high T |
| `cTPQ` | Canonical TPQ | Large systems |
| `OSS` | Optimal Spectrum Solver | All eigenvalues |
| `FTLM_GPU` | GPU-accelerated FTLM | Large systems with GPU |
| `LANCZOS_GPU` | GPU-accelerated Lanczos | Large ground state calcs |

### Command-Line Interface

```bash
./ED <hamiltonian_dir> [options]
```

#### Basic Options

| Option | Description | Default |
|--------|-------------|---------|
| `--method=<METHOD>` | Diagonalization method | `LANCZOS` |
| `--eigenvalues=<N>` | Number of eigenvalues | `1` |
| `--output=<DIR>` | Output directory | `./output` |
| `--config=<FILE>` | Configuration file | - |

#### System Options

| Option | Description | Default |
|--------|-------------|---------|
| `--num_sites=<N>` | Number of lattice sites | Auto-detect |
| `--spin_length=<S>` | Spin quantum number | `0.5` |
| `--fixed-sz` | Use fixed total Sz sector | Off |
| `--n_up=<N>` | Number of up spins (with --fixed-sz) | N/2 |

#### Workflow Options

| Option | Description |
|--------|-------------|
| `--standard` | Run standard diagonalization |
| `--symmetrized` | Use symmetry reduction |
| `--streaming-symmetry` | Stream symmetry sectors (memory-efficient) |
| `--thermo` | Compute thermodynamics from spectrum |
| `--dynamical-response` | Compute dynamical correlation functions |
| `--static-response` | Compute static susceptibilities |
| `--measure_spin` | Measure spin expectation values |

#### Thermal Options

| Option | Description | Default |
|--------|-------------|---------|
| `--temp_min=<T>` | Minimum temperature | `0.001` |
| `--temp_max=<T>` | Maximum temperature | `20.0` |
| `--temp_bins=<N>` | Number of temperature points | `100` |
| `--samples=<N>` | Random samples (FTLM/TPQ) | `40` |
| `--krylov_dim=<N>` | Krylov subspace dimension | `100` |

#### Example Commands

```bash
# Ground state with Lanczos
./ED ./ham_dir --method=LANCZOS --standard --eigenvalues=10

# Full spectrum for small system
./ED ./ham_dir --method=FULL --eigenvalues=FULL --thermo

# FTLM thermodynamics
./ED ./ham_dir --method=FTLM --samples=50 --krylov_dim=150 \
    --temp_min=0.01 --temp_max=10 --temp_bins=100

# GPU-accelerated FTLM
./ED ./ham_dir --method=FTLM_GPU --samples=100 --krylov_dim=200

# Fixed-Sz sector
./ED ./ham_dir --method=LANCZOS --fixed-sz --n_up=8 --eigenvalues=20

# Dynamical structure factor
./ED ./ham_dir --method=HYBRID --dynamical-response --dyn-thermal \
    --dyn-omega-min=-5 --dyn-omega-max=5 --dyn-points=1000

# With symmetries
./ED ./ham_dir --method=LANCZOS --symmetrized --eigenvalues=50
```

### Configuration Files

Configuration files provide reproducible parameter sets:

```ini
# ed_config.txt
[System]
num_sites = 16
spin_length = 0.5
hamiltonian_dir = ./pyrochlore_16

[Diagonalization]
method = FTLM
num_eigenvalues = 1
tolerance = 1e-10

[Thermal]
temp_min = 0.001
temp_max = 20.0
num_temp_bins = 100
num_samples = 50
ftlm_krylov_dim = 150

[Workflow]
output_dir = ./results/pyrochlore_ftlm
```

Load with: `./ED --config=ed_config.txt`

### Input File Formats

The ED executable expects Hamiltonian files in a specific directory structure:

```
hamiltonian_dir/
‚îú‚îÄ‚îÄ InterAll.dat          # Two-body interactions
‚îú‚îÄ‚îÄ Trans.dat             # Single-site terms (magnetic field)
‚îú‚îÄ‚îÄ ThreeBodyG.dat        # Three-body terms (optional)
‚îî‚îÄ‚îÄ pyrochlore_site_info.dat  # Site positions (for structure factors)
```

#### InterAll.dat (Two-Body Interactions)

```
# site_i site_j spin_op_i spin_op_j coupling_real coupling_imag
0 1 0 0 0.5 0.0    # S+_0 S-_1 term
0 1 1 1 0.5 0.0    # S-_0 S+_1 term
0 1 2 2 1.0 0.0    # Sz_0 Sz_1 term
...
```

Spin operators: 0 = S+, 1 = S-, 2 = Sz

#### Trans.dat (Single-Site Terms)

```
# site spin_op coupling_real coupling_imag
0 2 0.5 0.0    # 0.5 * Sz_0 (magnetic field)
1 2 0.5 0.0    # 0.5 * Sz_1
...
```

### Output Files

```
output/
‚îú‚îÄ‚îÄ eigenvalues.txt           # Eigenvalues (one per line)
‚îú‚îÄ‚îÄ ed_config.txt             # Resolved configuration
‚îú‚îÄ‚îÄ thermo/
‚îÇ   ‚îî‚îÄ‚îÄ thermo_data.txt       # T, E, C, S, F columns
‚îú‚îÄ‚îÄ dynamical_response/
‚îÇ   ‚îî‚îÄ‚îÄ Sqw_*.dat             # S(q,œâ) data files
‚îú‚îÄ‚îÄ static_response/
‚îÇ   ‚îî‚îÄ‚îÄ chi_*.dat             # œá(T) data files
‚îú‚îÄ‚îÄ eigenvectors/             # Eigenvector data (if computed)
‚îÇ   ‚îú‚îÄ‚îÄ eigenvalues.dat
‚îÇ   ‚îî‚îÄ‚îÄ eigenvector_*.dat
‚îî‚îÄ‚îÄ results.h5                # HDF5 output (all data)
```

---

## NLCE Workflow

### NLCE Overview

Numerical Linked Cluster Expansion (NLCE) computes bulk thermodynamic properties
by systematically summing contributions from finite clusters:

$$
P_\infty = \sum_c L(c) \cdot W_P(c)
$$

where:
- $P_\infty$ is the extensive property per site
- $L(c)$ is the lattice constant (multiplicity) of cluster $c$
- $W_P(c)$ is the weight of cluster $c$ for property $P$

The weight is computed via inclusion-exclusion:

$$
W_P(c) = P(c) - \sum_{s \subset c} W_P(s)
$$

### Workflow Components

```
workflows/nlce/
‚îú‚îÄ‚îÄ prep/
‚îÇ   ‚îî‚îÄ‚îÄ generate_pyrochlore_clusters.py  # Cluster enumeration
‚îú‚îÄ‚îÄ run/
‚îÇ   ‚îú‚îÄ‚îÄ nlce.py              # Full ED workflow orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ nlce_ftlm.py         # FTLM-based workflow
‚îÇ   ‚îú‚îÄ‚îÄ NLC_sum.py           # NLCE summation (full spectrum)
‚îÇ   ‚îî‚îÄ‚îÄ NLC_sum_ftlm.py      # NLCE summation (FTLM data)
‚îî‚îÄ‚îÄ analysis/
    ‚îú‚îÄ‚îÄ nlc_fit.py           # Fit NLCE to experimental data
    ‚îú‚îÄ‚îÄ nlc_fit_ftlm.py      # Fitting for FTLM results
    ‚îú‚îÄ‚îÄ nlc_convergence.py   # Order-by-order convergence
    ‚îî‚îÄ‚îÄ nlce_ftlm_convergence.py  # FTLM convergence analysis
```

### Running NLCE Calculations

#### Full Diagonalization Workflow

```bash
cd workflows/nlce/run

# Basic NLCE calculation (Heisenberg model)
python3 nlce.py --max_order=4 --Jxx=1.0 --Jyy=1.0 --Jzz=1.0 \
    --thermo --temp_min=0.01 --temp_max=10 --temp_bins=100

# With magnetic field
python3 nlce.py --max_order=4 --Jxx=1.0 --Jyy=1.0 --Jzz=1.0 \
    --h=0.5 --field_dir 0 0 1 --thermo

# Parallel execution
python3 nlce.py --max_order=5 --parallel --num_cores=16 \
    --Jxx=1.0 --Jyy=1.0 --Jzz=1.0 --thermo

# Skip certain steps (resume interrupted run)
python3 nlce.py --max_order=4 --skip_cluster_gen --skip_ham_prep \
    --Jxx=1.0 --Jyy=1.0 --Jzz=1.0 --thermo
```

#### nlce.py Options

| Option | Description | Default |
|--------|-------------|---------|
| `--max_order` | Maximum cluster order | Required |
| `--base_dir` | Output directory | `./nlce_results` |
| `--ed_executable` | Path to ED binary | `../../../build/ED` |
| `--Jxx, --Jyy, --Jzz` | Exchange couplings | `1.0` |
| `--h` | Magnetic field strength | `0.0` |
| `--field_dir` | Field direction (x,y,z) | `[1/‚àö3, 1/‚àö3, 1/‚àö3]` |
| `--method` | ED method (`FULL`, `OSS`, `mTPQ`) | `FULL` |
| `--thermo` | Compute thermodynamics | Off |
| `--temp_min/max/bins` | Temperature grid | `0.001, 20.0, 100` |
| `--parallel` | Enable parallel execution | Off |
| `--num_cores` | CPU cores for parallel | All available |
| `--symmetrized` | Use symmetry reduction | Off |
| `--measure_spin` | Measure ‚ü®S‚ü© values | Off |
| `--skip_cluster_gen` | Skip cluster generation | Off |
| `--skip_ham_prep` | Skip Hamiltonian prep | Off |
| `--skip_ed` | Skip ED calculations | Off |
| `--skip_nlc` | Skip NLCE summation | Off |

### NLCE with FTLM

For larger clusters (>15 sites), use FTLM instead of full diagonalization:

```bash
# FTLM-based NLCE
python3 nlce_ftlm.py --max_order=6 --Jxx=1.0 --Jyy=1.0 --Jzz=1.0 \
    --ftlm_samples=50 --krylov_dim=200 \
    --temp_min=0.01 --temp_max=10 --temp_bins=100

# With GPU acceleration
python3 nlce_ftlm.py --max_order=6 --ftlm_samples=100 --krylov_dim=300 \
    --Jxx=1.0 --Jyy=1.0 --Jzz=1.0
```

#### nlce_ftlm.py Additional Options

| Option | Description | Default |
|--------|-------------|---------|
| `--ftlm_samples` | Random samples per cluster | `40` |
| `--krylov_dim` | Krylov subspace dimension | `150` |
| `--resummation` | Series acceleration method | `auto` |
| `--robust_pipeline` | Cross-validated C(T) | Off |

### NLCE Output Structure

```
nlce_results/
‚îú‚îÄ‚îÄ clusters_order_N/
‚îÇ   ‚îî‚îÄ‚îÄ cluster_info_order_N/
‚îÇ       ‚îú‚îÄ‚îÄ cluster_0_order_1.dat
‚îÇ       ‚îú‚îÄ‚îÄ cluster_1_order_2.dat
‚îÇ       ‚îî‚îÄ‚îÄ subclusters_info.txt
‚îú‚îÄ‚îÄ hamiltonians_order_N/
‚îÇ   ‚îî‚îÄ‚îÄ cluster_X_order_Y/
‚îÇ       ‚îú‚îÄ‚îÄ InterAll.dat
‚îÇ       ‚îú‚îÄ‚îÄ Trans.dat
‚îÇ       ‚îî‚îÄ‚îÄ pyrochlore_site_info.dat
‚îú‚îÄ‚îÄ ed_results_order_N/
‚îÇ   ‚îî‚îÄ‚îÄ cluster_X_order_Y/
‚îÇ       ‚îî‚îÄ‚îÄ output/
‚îÇ           ‚îú‚îÄ‚îÄ eigenvalues.txt
‚îÇ           ‚îî‚îÄ‚îÄ thermo/
‚îú‚îÄ‚îÄ nlc_results_order_N/
‚îÇ   ‚îú‚îÄ‚îÄ specific_heat.dat
‚îÇ   ‚îú‚îÄ‚îÄ entropy.dat
‚îÇ   ‚îú‚îÄ‚îÄ energy.dat
‚îÇ   ‚îî‚îÄ‚îÄ nlce_convergence.png
‚îú‚îÄ‚îÄ thermo_plots_order_N/
‚îî‚îÄ‚îÄ nlce_workflow.log
```

### Analysis and Fitting

#### Convergence Analysis

```bash
cd workflows/nlce/analysis

# Check order-by-order convergence
python3 nlc_convergence.py \
    --cluster_dir ../run/nlce_results/clusters_order_5/cluster_info_order_5 \
    --eigenvalue_dir ../run/nlce_results/ed_results_order_5 \
    --output_dir ./convergence_analysis \
    --temp_min=0.1 --temp_max=10

# FTLM convergence
python3 nlce_ftlm_convergence.py \
    --cluster_dir ../run/nlce_ftlm_results/clusters_order_6 \
    --ftlm_dir ../run/nlce_ftlm_results/ftlm_results_order_6 \
    --output_dir ./ftlm_convergence
```

#### Fitting to Experimental Data

```bash
# Fit exchange parameters to specific heat data
python3 nlc_fit.py \
    --exp_data specific_heat_experiment.txt \
    --max_order 4 \
    --Jxx_range 0.5 1.5 \
    --Jyy_range 0.5 1.5 \
    --Jzz_range 0.5 1.5 \
    --optimizer differential_evolution \
    --output_dir ./fitting_results

# Multi-field fitting
python3 nlc_fit.py \
    --exp_data_config multi_field_config.json \
    --max_order 4 \
    --optimizer basinhopping
```

#### Fitting Configuration (JSON)

```json
[
  {
    "file": "specific_heat_0T.txt",
    "h": 0.0,
    "field_dir": [0, 0, 1],
    "weight": 1.0,
    "temp_min": 0.5,
    "temp_max": 10.0
  },
  {
    "file": "specific_heat_4T.txt",
    "h": 4.0,
    "field_dir": [0, 0, 1],
    "weight": 0.5
  }
]
```

---

## Advanced Topics

### Large System Calculations (28-32 Sites)

For systems with 28+ sites, special strategies are required:

```bash
# 1. Check resource requirements
python3 scripts/check_system_feasibility.py 32 --fixed-sz --method=FTLM

# 2. Use Fixed-Sz + FTLM
./ED ./ham_dir --method=FTLM --fixed-sz --samples=50 \
    --krylov_dim=200 --thermo

# 3. Memory-efficient streaming symmetry
./ED ./ham_dir --method=LANCZOS --streaming-symmetry --eigenvalues=10
```

Key strategies:
- **Skip spatial symmetries** (construction too expensive)
- **Use Fixed-Sz** (reduces 2¬≥¬≤ ‚Üí 600M states)
- **Use FTLM/TPQ** (no eigenvector storage)
- **Prefer CPU over GPU** (32 sites needs 27-50 GB GPU memory)

### Temperature Scan Optimization

Multi-temperature dynamical correlations are **up to 35√ó faster** by reusing
the Lanczos decomposition across temperatures. Enable automatically for
temperature scans with `--dyn-thermal --temp_bins>1`.

### GPU Acceleration

```bash
# GPU-accelerated FTLM
./ED ./ham_dir --method=FTLM_GPU --samples=100 --krylov_dim=400

# GPU dynamical response
./ED ./ham_dir --method=HYBRID --dynamical-response --dyn-use-gpu
```

Requires: CUDA build (`-DWITH_CUDA=ON`) and GPU with sufficient memory.

### MPI Parallel TPQ

```bash
# Run TPQ with MPI parallelization over samples
mpirun -np 16 ./ED ./ham_dir --method=mTPQ --samples=160 --thermo
```

Each MPI rank processes samples/size samples independently.

---

## Python Utilities

### python/edlib Package

```python
from edlib import helper_pyrochlore, hdf5_io

# Generate pyrochlore Hamiltonian
helper_pyrochlore.generate_hamiltonian(
    output_dir="./ham",
    Jxx=1.0, Jyy=1.0, Jzz=1.0,
    h=0.0, field_dir=[0, 0, 1]
)

# Read HDF5 results
with hdf5_io.open_results("./output/results.h5") as f:
    eigenvalues = f.get_eigenvalues()
    temps, cv = f.get_thermodynamics("specific_heat")
```

### Plotting Scripts

```bash
cd scripts

# Plot thermodynamics
python3 plot_ftlm.py --input ../results/thermo/thermo_data.txt

# Animate dynamical structure factor
python3 animate_DSSF.py --input ../results/dynamical_response/

# Plot NLCE convergence
python3 plot_ftlm_clusters.py --cluster_dir ../workflows/nlce/run/nlce_results
```

---

## Recent Updates

**üöÄ Temperature Scan Optimization** ‚Äì Dynamical correlations at multiple
temperatures now run **up to 35√ó faster** by reusing the Lanczos decomposition.

**‚öôÔ∏è Large System Support (32+ Sites)** ‚Äì Fixed-Sz + FTLM methods enable
calculations on 600M-dimensional Hilbert spaces with ~40-80 GB RAM.

**üì¶ Reorganized Codebase** ‚Äì Modern directory layout with separated headers
(`include/ed/`), sources (`src/`), Python package (`python/edlib/`), and
workflows (`workflows/nlce/`).

---

## Getting Help

- `ED --help` ‚Äì Full option reference
- `ED --method-info=<METHOD>` ‚Äì Method-specific parameters
- `docs/` ‚Äì Extended documentation
- `examples/` ‚Äì Sample configuration files

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
